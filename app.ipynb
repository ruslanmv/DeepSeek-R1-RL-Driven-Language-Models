{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOnKkTul/Cl35D9QTLia2GL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/DeepSeek-R1-RL-Driven-Language-Models/blob/master/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "def clone_and_extract(repo_url):\n",
        "    \"\"\"\n",
        "    Clones a Git repository and extracts its contents into the current directory.\n",
        "\n",
        "    Args:\n",
        "    repo_url: The URL of the Git repository to clone.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Clone the repository into a temporary directory\n",
        "        temp_dir = \"temp_repo\"\n",
        "        subprocess.run(['git', 'clone', repo_url, temp_dir], check=True)\n",
        "\n",
        "        # Move the contents of the cloned repository to the current directory\n",
        "        for item in os.listdir(temp_dir):\n",
        "            src = os.path.join(temp_dir, item)\n",
        "            dst = os.path.join('.', item)\n",
        "            shutil.move(src, dst)\n",
        "\n",
        "        # Remove the temporary directory\n",
        "        shutil.rmtree(temp_dir)\n",
        "\n",
        "        print(f\"Successfully cloned and extracted repository: {repo_url}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error cloning or extracting repository: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    repo_url = \"https://github.com/ruslanmv/DeepSeek-R1-RL-Driven-Language-Models\"\n",
        "    clone_and_extract(repo_url)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def install_dependencies_and_clear_outputs():\n",
        "    \"\"\"\n",
        "    Installs dependencies from a requirements.txt file and clears the notebook outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Install dependencies\n",
        "        subprocess.run(['pip', 'install', '-r', 'requirements.txt'], check=True)\n",
        "\n",
        "        # Clear notebook outputs\n",
        "        from IPython.display import clear_output\n",
        "        clear_output()\n",
        "\n",
        "        print(\"Successfully installed dependencies and cleared notebook outputs.\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error installing dependencies: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies_and_clear_outputs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG6X9s0Q1aqv",
        "outputId": "2dbca979-b24d-4b8e-b5bb-a2d96713da4c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed dependencies and cleared notebook outputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import spaces\n",
        "from transformers import GemmaTokenizer, AutoModelForCausalLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "# Set an environment variable\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
        "\n",
        "# Custom HTML for the header and footer\n",
        "DESCRIPTION = '''\n",
        "<div style=\"text-align: center;\">\n",
        "    <h1 style=\"font-size: 32px; font-weight: bold; color: #1565c0;\">DeepSeek-R1-Distill-Qwen-32B-bnb-4bit</h1>\n",
        "    <p style=\"font-size: 16px; color: #555;\">Developed by <a href=\"https://ruslanmv.com/\" target=\"_blank\" style=\"color: #1565c0; text-decoration: none;\">RuslanMV</a></p>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "FOOTER = '''\n",
        "<div style=\"text-align: center; margin-top: 20px; padding: 10px; background-color: #f5f5f5; border-radius: 8px;\">\n",
        "    <p style=\"font-size: 14px; color: #777;\">Powered by Gradio and Hugging Face Transformers</p>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "PLACEHOLDER = '''\n",
        "<div style=\"padding: 30px; text-align: center; display: flex; flex-direction: column; align-items: center;\">\n",
        "    <h1 style=\"font-size: 28px; margin-bottom: 2px; opacity: 0.55;\">DeepSeek-R1-Distill-Qwen-32B-bnb-4bit</h1>\n",
        "    <p style=\"font-size: 18px; margin-bottom: 2px; opacity: 0.65;\">Ask me anything...</p>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "# Custom CSS for better styling\n",
        "css = \"\"\"\n",
        "h1 {\n",
        "    text-align: center;\n",
        "    display: block;\n",
        "    font-weight: bold;\n",
        "    color: #1565c0;\n",
        "}\n",
        "#duplicate-button {\n",
        "    margin: auto;\n",
        "    color: white;\n",
        "    background: #1565c0;\n",
        "    border-radius: 100vh;\n",
        "}\n",
        ".chatbot {\n",
        "    border-radius: 8px;\n",
        "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        ".accordion {\n",
        "    background-color: #f5f5f5;\n",
        "    border-radius: 8px;\n",
        "    padding: 10px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit\")\n",
        "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<|user|>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<|assistant|>' + tool['type'] + ':' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '}}\\\\n'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<|assistant|>' + tool['type'] + ':' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '}}\\\\n'}}{{'}}\\\\n'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<|assistant|>' + message['content'] + '}}\\\\n'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<|assistant|>' + content + '}}\\\\n'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<|tool|>' + message['content'] + '}}\\\\n'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<|tool|>' + message['content'] + '}}\\\\n'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<|assistant|>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<|assistant|>'}}{% endif %}\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit\", device_map=\"auto\")\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "]\n",
        "\n",
        "@spaces.GPU(duration=120)\n",
        "def chat_llama3_8b(message: str, history: list, temperature: float, max_new_tokens: int) -> str:\n",
        "    \"\"\"\n",
        "    Generate a streaming response using the llama3-8b model.\n",
        "    Args:\n",
        "        message (str): The input message.\n",
        "        history (list): The conversation history used by ChatInterface.\n",
        "        temperature (float): The temperature for generating the response.\n",
        "        max_new_tokens (int): The maximum number of new tokens to generate.\n",
        "    Returns:\n",
        "        str: The generated response.\n",
        "    \"\"\"\n",
        "    conversation = []\n",
        "    for user, assistant in history:\n",
        "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
        "    conversation.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)\n",
        "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        eos_token_id=terminators,\n",
        "    )\n",
        "    if temperature == 0:\n",
        "        generate_kwargs['do_sample'] = False\n",
        "\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    outputs = []\n",
        "    for text in streamer:\n",
        "        if \"<think>\" in text:\n",
        "            text = text.replace(\"<think>\", \"[think]\").strip()\n",
        "        if \"</think>\" in text:\n",
        "            text = text.replace(\"</think>\", \"[/think]\").strip()\n",
        "        outputs.append(text)\n",
        "        yield \"\".join(outputs)\n",
        "\n",
        "# Gradio block\n",
        "chatbot = gr.Chatbot(height=450, placeholder=PLACEHOLDER, label='Chat with DeepSeek-R1')\n",
        "\n",
        "with gr.Blocks(fill_height=True, css=css) as demo:\n",
        "    gr.Markdown(DESCRIPTION)\n",
        "    gr.ChatInterface(\n",
        "        fn=chat_llama3_8b,\n",
        "        chatbot=chatbot,\n",
        "        fill_height=True,\n",
        "        additional_inputs_accordion=gr.Accordion(label=\"⚙️ Parameters\", open=False, render=False),\n",
        "        additional_inputs=[\n",
        "            gr.Slider(minimum=0, maximum=1, step=0.1, value=0.5, label=\"Temperature\", render=False),\n",
        "            gr.Slider(minimum=128, maximum=4096, step=1, value=1024, label=\"Max new tokens\", render=False),\n",
        "        ],\n",
        "\n",
        "\n",
        "        examples=[\n",
        "            ['Write a short poem about a lonely robot finding a friend.'],\n",
        "            ['Explain quantum mechanics as if I’m a beginner in high school physics.'],\n",
        "            ['If you have three apples and cut each into four pieces, how many pieces do you have?'],\n",
        "            ['Make up a funny conversation between a cat and a goldfish.'],\n",
        "            ['Convince me that dragons could exist in some form.'],\n",
        "            ['What is the square root of 3,456 rounded to two decimal places?'],\n",
        "            ['If humans had three arms, how would it change sports like basketball?'],\n",
        "            ['Do you think artificial intelligence can ever truly be creative? Why or why not?'],\n",
        "            ['Imagine a futuristic city powered entirely by renewable energy. What would it look like?'],\n",
        "            ['Write a sentence where every word starts with the letter \"S\".'],\n",
        "            ['Describe a traditional dish from Japan and how it is made.'],\n",
        "            ['Is it ethical to use cloning to bring back extinct species? Why or why not?'],\n",
        "            ['Write a Python function to reverse a string.'],\n",
        "            ['Give me a motivational speech for finishing a challenging project.'],\n",
        "            ['If dogs ruled the world, what laws would they make?']\n",
        "        ]\n",
        "\n",
        "\n",
        "        ,\n",
        "        cache_examples=False,\n",
        "    )\n",
        "    gr.Markdown(FOOTER)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "1YDoH_fdPQOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional if you Want run with NGROK"
      ],
      "metadata": {
        "id": "cxab4i7ZQGqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# SINGLE-CELL SNIPPET\n",
        "# ===================\n",
        "\n",
        "# 1. Install packages quietly\n",
        "!pip install gradio pyngrok python-dotenv -q\n",
        "\n",
        "# 2. Import libraries\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Detect Google Colab\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# 3. Load NGROK_TOKEN\n",
        "if IN_COLAB:\n",
        "    NGROK_AUTH_TOKEN = userdata.get('NGROK_TOKEN')\n",
        "    if not NGROK_AUTH_TOKEN:\n",
        "        raise ValueError(\"Please set the 'NGROK_TOKEN' in your Google Colab secrets.\")\n",
        "else:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    NGROK_AUTH_TOKEN = os.getenv(\"NGROK_TOKEN\")\n",
        "    if not NGROK_AUTH_TOKEN:\n",
        "        raise ValueError(\"Please set the 'NGROK_TOKEN' in your .env file.\")\n",
        "\n",
        "# Set ngrok auth token\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# 4. Create a function to start the Gradio app from app.py\n",
        "def run_app():\n",
        "    process = subprocess.Popen(\n",
        "        [\"python\", \"app.py\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    # Continuously read and print the logs\n",
        "    for line in process.stdout:\n",
        "        print(line, end=\"\")\n",
        "    for line in process.stderr:\n",
        "        print(line, end=\"\")\n",
        "\n",
        "# 5. Start the Gradio app in a background thread\n",
        "thread = Thread(target=run_app)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "# Add a delay to allow the Gradio app to start\n",
        "time.sleep(10)\n",
        "\n",
        "# 6. Connect ngrok to port 7860 (Gradio's default port)\n",
        "# Check if a tunnel is already active and disconnect it\n",
        "tunnels = ngrok.get_tunnels()\n",
        "if tunnels:\n",
        "    print(\"Existing tunnel found, disconnecting:\", tunnels[0].public_url)\n",
        "    ngrok.disconnect(tunnels[0].public_url)\n",
        "public_url = ngrok.connect(7860)\n",
        "print(\"Your Gradio app is publicly available at:\", public_url)\n",
        "\n",
        "# 7. Function to handle ngrok tunnel termination\n",
        "def wait_for_termination():\n",
        "    print(\"Type 'exit' to terminate the ngrok tunnel.\")\n",
        "    while True:\n",
        "        user_input = input(\"Command: \").strip().lower()\n",
        "        if user_input == 'exit':\n",
        "            print(\"Terminating ngrok tunnel...\")\n",
        "            ngrok.disconnect(public_url.public_url)\n",
        "            print(\"Ngrok tunnel terminated.\")\n",
        "            break\n",
        "\n",
        "# 8. Start the termination-watching function\n",
        "wait_for_termination()"
      ],
      "metadata": {
        "id": "D1jCWw4JSGhz",
        "outputId": "c89770a0-2cd1-4a93-acb9-dc7c4232805c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Gradio app is publicly available at: NgrokTunnel: \"https://60f1-34-126-82-198.ngrok-free.app\" -> \"http://localhost:7860\"\n",
            "Type 'exit' to terminate the ngrok tunnel.\n"
          ]
        }
      ]
    }
  ]
}